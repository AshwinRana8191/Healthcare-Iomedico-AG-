{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oncology Clinical Trials Model Evaluation\n",
    "\n",
    "This notebook demonstrates the model evaluation process for oncology clinical trial outcome prediction. We'll evaluate the models trained in the previous notebook, visualize their performance, and interpret the results to gain insights into factors affecting clinical trial outcomes.\n",
    "\n",
    "This completes our analysis workflow from data exploration to feature engineering, model training, and now model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root to path to import project modules\n",
    "project_root = Path().resolve().parents[0]\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import model evaluation functions\n",
    "from src.models.evaluate_model import (\n",
    "    load_latest_model,\n",
    "    load_test_data,\n",
    "    evaluate_classification_model,\n",
    "    evaluate_regression_model,\n",
    "    plot_feature_importance,\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curve,\n",
    "    plot_precision_recall_curve,\n",
    "    plot_residuals,\n",
    "    plot_prediction_error,\n",
    "    generate_evaluation_report\n",
    ")\n",
    "\n",
    "# Import visualization functions\n",
    "from src.visualization.visualize import set_plotting_style\n",
    "\n",
    "# Define project directories\n",
    "PROJECT_DIR = project_root\n",
    "PROCESSED_DATA_DIR = PROJECT_DIR / 'data' / 'processed'\n",
    "MODEL_DIR = PROJECT_DIR / 'models'\n",
    "REPORT_DIR = PROJECT_DIR / 'reports'\n",
    "FIGURE_DIR = PROJECT_DIR / 'reports' / 'figures'\n",
    "\n",
    "# Set plotting style\n",
    "set_plotting_style()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data\n",
    "\n",
    "Let's load the modeling-ready dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recent modeling-ready dataset\n",
    "df_test = load_test_data()\n",
    "\n",
    "print(f\"Loaded test dataset with {df_test.shape[0]} rows and {df_test.shape[1]} columns\")\n",
    "\n",
    "# Display the first few rows\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Data\n",
    "\n",
    "Let's prepare the test data for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variables\n",
    "classification_target = 'is_completed'  # Binary: 1 for completed, 0 for terminated\n",
    "regression_target = 'duration_days'     # Continuous: trial duration in days\n",
    "\n",
    "# Filter data to include only completed or terminated trials for classification\n",
    "df_classification = df_test[df_test['overall_status'].isin(['Completed', 'Terminated'])]\n",
    "\n",
    "# Create binary target for classification (1 for completed, 0 for terminated)\n",
    "df_classification[classification_target] = (df_classification['overall_status'] == 'Completed').astype(int)\n",
    "\n",
    "# Filter data to include only completed trials for regression (duration prediction)\n",
    "df_regression = df_test[df_test['overall_status'] == 'Completed']\n",
    "\n",
    "# Identify feature columns (exclude target variables and metadata)\n",
    "exclude_cols = ['nct_id', 'overall_status', classification_target, regression_target]\n",
    "feature_cols = [col for col in df_test.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Classification test set: {df_classification.shape[0]} rows, {len(feature_cols)} features\")\n",
    "print(f\"Regression test set: {df_regression.shape[0]} rows, {len(feature_cols)} features\")\n",
    "\n",
    "# Prepare feature matrices and target vectors\n",
    "X_classification = df_classification[feature_cols]\n",
    "y_classification = df_classification[classification_target]\n",
    "\n",
    "X_regression = df_regression[feature_cols]\n",
    "y_regression = df_regression[regression_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Classification Model\n",
    "\n",
    "Let's load and evaluate the best classification model trained in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recent classification model\n",
    "classification_model_dict = load_latest_model('classification')\n",
    "\n",
    "# Extract model components\n",
    "classification_model = classification_model_dict['model']\n",
    "classification_preprocessor = classification_model_dict['preprocessor']\n",
    "\n",
    "print(f\"Loaded classification model: {type(classification_model).__name__}\")\n",
    "\n",
    "# Evaluate the model\n",
    "classification_eval_results = evaluate_classification_model(\n",
    "    classification_model_dict, X_classification, y_classification\n",
    ")\n",
    "\n",
    "# Display performance metrics\n",
    "print(\"\nClassification Model Performance:\")\n",
    "for metric, value in classification_eval_results['metrics'].items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"\nClassification Report:\")\n",
    "print(classification_eval_results['classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Classification Results\n",
    "\n",
    "Let's create visualizations to better understand the classification model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_confusion_matrix(\n",
    "    classification_eval_results['confusion_matrix'],\n",
    "    classes=['Terminated', 'Completed'],\n",
    "    title='Confusion Matrix for Trial Completion Prediction'\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_roc_curve(\n",
    "    classification_eval_results['fpr'],\n",
    "    classification_eval_results['tpr'],\n",
    "    classification_eval_results['metrics']['roc_auc'],\n",
    "    title='ROC Curve for Trial Completion Prediction'\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_recall_curve(\n",
    "    classification_eval_results['precision'],\n",
    "    classification_eval_results['recall'],\n",
    "    classification_eval_results['metrics']['average_precision'],\n",
    "    title='Precision-Recall Curve for Trial Completion Prediction'\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance for Classification\n",
    "\n",
    "Let's examine which features are most important for predicting trial completion status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "if 'feature_importance' in classification_eval_results:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_feature_importance(\n",
    "        classification_eval_results['feature_importance'],\n",
    "        title='Feature Importance for Trial Completion Prediction',\n",
    "        top_n=20\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display top 10 features and their importance scores\n",
    "    print(\"\nTop 10 Features for Trial Completion Prediction:\")\n",
    "    for feature, importance in classification_eval_results['feature_importance'].items()[:10]:\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "else:\n",
    "    print(\"Feature importance not available for this model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Analysis for Classification\n",
    "\n",
    "Let's use SHAP (SHapley Additive exPlanations) to better understand how each feature contributes to the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP analysis (if available)\n",
    "if 'shap_values' in classification_eval_results:\n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(\n",
    "        classification_eval_results['shap_values'],\n",
    "        X_classification,\n",
    "        feature_names=feature_cols,\n",
    "        plot_type='bar'\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed SHAP plot for top features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(\n",
    "        classification_eval_results['shap_values'],\n",
    "        X_classification,\n",
    "        feature_names=feature_cols\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"SHAP analysis not available for this model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Regression Model\n",
    "\n",
    "Now, let's load and evaluate the best regression model trained in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recent regression model\n",
    "regression_model_dict = load_latest_model('regression')\n",
    "\n",
    "# Extract model components\n",
    "regression_model = regression_model_dict['model']\n",
    "regression_preprocessor = regression_model_dict['preprocessor']\n",
    "\n",
    "print(f\"Loaded regression model: {type(regression_model).__name__}\")\n",
    "\n",
    "# Evaluate the model\n",
    "regression_eval_results = evaluate_regression_model(\n",
    "    regression_model_dict, X_regression, y_regression\n",
    ")\n",
    "\n",
    "# Display performance metrics\n",
    "print(\"\nRegression Model Performance:\")\n",
    "for metric, value in regression_eval_results['metrics'].items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Regression Results\n",
    "\n",
    "Let's create visualizations to better understand the regression model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs. predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_prediction_error(\n",
    "    regression_eval_results['y_true'],\n",
    "    regression_eval_results['y_pred'],\n",
    "    title='Actual vs. Predicted Trial Duration'\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_residuals(\n",
    "    regression_eval_results['y_true'],\n",
    "    regression_eval_results['y_pred'],\n",
    "    title='Residuals for Trial Duration Prediction'\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot residual distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "residuals = regression_eval_results['y_true'] - regression_eval_results['y_pred']\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.xlabel('Residual (Actual - Predicted)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance for Regression\n",
    "\n",
    "Let's examine which features are most important for predicting trial duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "if 'feature_importance' in regression_eval_results:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_feature_importance(\n",
    "        regression_eval_results['feature_importance'],\n",
    "        title='Feature Importance for Trial Duration Prediction',\n",
    "        top_n=20\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display top 10 features and their importance scores\n",
    "    print(\"\nTop 10 Features for Trial Duration Prediction:\")\n",
    "    for feature, importance in regression_eval_results['feature_importance'].items()[:10]:\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "else:\n",
    "    print(\"Feature importance not available for this model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Analysis for Regression\n",
    "\n",
    "Let's use SHAP to better understand how each feature contributes to the regression model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP analysis (if available)\n",
    "if 'shap_values' in regression_eval_results:\n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(\n",
    "        regression_eval_results['shap_values'],\n",
    "        X_regression,\n",
    "        feature_names=feature_cols,\n",
    "        plot_type='bar'\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed SHAP plot for top features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(\n",
    "        regression_eval_results['shap_values'],\n",
    "        X_regression,\n",
    "        feature_names=feature_cols\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"SHAP analysis not available for this model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Evaluation Report\n",
    "\n",
    "Let's generate a comprehensive evaluation report for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate evaluation report\n",
    "report = generate_evaluation_report(\n",
    "    classification_model_dict,\n",
    "    classification_eval_results,\n",
    "    regression_model_dict,\n",
    "    regression_eval_results\n",
    ")\n",
    "\n",
    "# Display report summary\n",
    "print(\"\nEvaluation Report Summary:\")\n",
    "print(report['summary'])\n",
    "\n",
    "# Save report to file\n",
    "report_path = REPORT_DIR / f\"model_evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html\"\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report['html'])\n",
    "\n",
    "print(f\"\nSaved evaluation report to {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights and Findings\n",
    "\n",
    "Based on our model evaluation, we can draw several insights about oncology clinical trials:\n",
    "\n",
    "### Factors Affecting Trial Completion\n",
    "\n",
    "1. **[Add insights after running the notebook]**\n",
    "2. **[Add insights after running the notebook]**\n",
    "3. **[Add insights after running the notebook]**\n",
    "\n",
    "### Factors Affecting Trial Duration\n",
    "\n",
    "1. **[Add insights after running the notebook]**\n",
    "2. **[Add insights after running the notebook]**\n",
    "3. **[Add insights after running the notebook]**\n",
    "\n",
    "### Recommendations for Trial Design\n",
    "\n",
    "1. **[Add recommendations after running the notebook]**\n",
    "2. **[Add recommendations after running the notebook]**\n",
    "3. **[Add recommendations after running the notebook]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've evaluated the models trained for predicting oncology clinical trial outcomes:\n",
    "\n",
    "1. **Classification Model for Trial Completion Status**\n",
    "   - Evaluated model performance using accuracy, precision, recall, F1 score, and ROC AUC\n",
    "   - Visualized the confusion matrix, ROC curve, and precision-recall curve\n",
    "   - Analyzed feature importance and SHAP values to understand factors affecting trial completion\n",
    "\n",
    "2. **Regression Model for Trial Duration**\n",
    "   - Evaluated model performance using R², MAE, and RMSE\n",
    "   - Visualized actual vs. predicted values and residuals\n",
    "   - Analyzed feature importance and SHAP values to understand factors affecting trial duration\n",
    "\n",
    "3. **Generated a comprehensive evaluation report** that can be shared with stakeholders\n",
    "\n",
    "This completes our analysis workflow from data exploration to feature engineering, model training, and model evaluation. The insights gained from this analysis can help inform the design and planning of future oncology clinical trials to improve their success rates and efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}