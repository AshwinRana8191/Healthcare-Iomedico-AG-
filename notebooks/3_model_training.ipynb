{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oncology Clinical Trials Model Training\n",
    "\n",
    "This notebook demonstrates the model training process for predicting oncology clinical trial outcomes. We'll train both classification models (to predict trial completion status) and regression models (to predict trial duration).\n",
    "\n",
    "Building on the feature engineering performed in the previous notebook, we'll now use those features to train predictive models and analyze their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root to path to import project modules\n",
    "project_root = Path().resolve().parents[0]\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import model training functions\n",
    "from src.models.train_model import (\n",
    "    load_modeling_data,\n",
    "    train_classification_models,\n",
    "    train_regression_models,\n",
    "    save_model\n",
    ")\n",
    "\n",
    "# Import visualization functions for model analysis\n",
    "from src.visualization.visualize import set_plotting_style\n",
    "\n",
    "# Define project directories\n",
    "PROJECT_DIR = project_root\n",
    "PROCESSED_DATA_DIR = PROJECT_DIR / 'data' / 'processed'\n",
    "MODEL_DIR = PROJECT_DIR / 'models'\n",
    "\n",
    "# Set plotting style\n",
    "set_plotting_style()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Modeling Data\n",
    "\n",
    "Let's load the modeling-ready dataset that was prepared in the feature engineering notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recent modeling-ready dataset\n",
    "df_modeling = load_modeling_data()\n",
    "\n",
    "print(f\"Loaded dataset with {df_modeling.shape[0]} rows and {df_modeling.shape[1]} columns\")\n",
    "\n",
    "# Display the first few rows\n",
    "df_modeling.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Features and Target Variables\n",
    "\n",
    "We'll prepare two sets of models:\n",
    "1. Classification models to predict trial completion status (completed vs. terminated)\n",
    "2. Regression models to predict trial duration (in days)\n",
    "\n",
    "Let's prepare the feature matrix and target variables for both tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variables\n",
    "classification_target = 'is_completed'  # Binary: 1 for completed, 0 for terminated\n",
    "regression_target = 'duration_days'     # Continuous: trial duration in days\n",
    "\n",
    "# Filter data to include only completed or terminated trials for classification\n",
    "df_classification = df_modeling[df_modeling['overall_status'].isin(['Completed', 'Terminated'])]\n",
    "\n",
    "# Create binary target for classification (1 for completed, 0 for terminated)\n",
    "df_classification[classification_target] = (df_classification['overall_status'] == 'Completed').astype(int)\n",
    "\n",
    "# Filter data to include only completed trials for regression (duration prediction)\n",
    "df_regression = df_modeling[df_modeling['overall_status'] == 'Completed']\n",
    "\n",
    "# Identify feature columns (exclude target variables and metadata)\n",
    "exclude_cols = ['nct_id', 'overall_status', classification_target, regression_target]\n",
    "feature_cols = [col for col in df_modeling.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Classification dataset: {df_classification.shape[0]} rows, {len(feature_cols)} features\")\n",
    "print(f\"Regression dataset: {df_regression.shape[0]} rows, {len(feature_cols)} features\")\n",
    "\n",
    "# Prepare feature matrices and target vectors\n",
    "X_classification = df_classification[feature_cols]\n",
    "y_classification = df_classification[classification_target]\n",
    "\n",
    "X_regression = df_regression[feature_cols]\n",
    "y_regression = df_regression[regression_target]\n",
    "\n",
    "# Check class balance for classification\n",
    "print(\"\nClass distribution for completion status:\")\n",
    "print(y_classification.value_counts(normalize=True).round(3) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classification Models\n",
    "\n",
    "Now, let's train classification models to predict trial completion status. We'll train multiple models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classification models\n",
    "classification_results = train_classification_models(X_classification, y_classification)\n",
    "\n",
    "# Display performance metrics\n",
    "classification_metrics = pd.DataFrame(classification_results['metrics'])\n",
    "classification_metrics = classification_metrics.sort_values('f1_score', ascending=False)\n",
    "\n",
    "print(\"\nClassification Model Performance:\")\n",
    "display(classification_metrics)\n",
    "\n",
    "# Identify best model\n",
    "best_classification_model_name = classification_metrics.index[0]\n",
    "best_classification_model = classification_results['models'][best_classification_model_name]\n",
    "\n",
    "print(f\"\nBest classification model: {best_classification_model_name}\")\n",
    "\n",
    "# Save the best model\n",
    "model_info = {\n",
    "    'model': best_classification_model,\n",
    "    'preprocessor': classification_results['preprocessor'],\n",
    "    'feature_names': feature_cols,\n",
    "    'metrics': classification_metrics.loc[best_classification_model_name].to_dict()\n",
    "}\n",
    "\n",
    "save_model(model_info, 'classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Classification Results\n",
    "\n",
    "Let's visualize the performance of our classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model performance comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot accuracy, precision, recall, and F1 score\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "\n",
    "# Prepare data for plotting\n",
    "plot_data = classification_metrics[metrics_to_plot].reset_index()\n",
    "plot_data = pd.melt(plot_data, id_vars=['index'], value_vars=metrics_to_plot, \n",
    "                    var_name='Metric', value_name='Score')\n",
    "\n",
    "# Create the plot\n",
    "sns.barplot(x='index', y='Score', hue='Metric', data=plot_data)\n",
    "plt.title('Classification Model Performance Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title='Metric')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance for Classification\n",
    "\n",
    "Let's examine which features are most important for predicting trial completion status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from the best model (if available)\n",
    "if hasattr(best_classification_model, 'feature_importances_'):\n",
    "    # Get feature importances\n",
    "    importances = best_classification_model.feature_importances_\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    if hasattr(classification_results['preprocessor'], 'get_feature_names_out'):\n",
    "        feature_names = classification_results['preprocessor'].get_feature_names_out()\n",
    "    else:\n",
    "        feature_names = feature_cols\n",
    "    \n",
    "    # Create a DataFrame for visualization\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot top 20 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "    plt.title(f'Top 20 Important Features for {best_classification_model_name}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Feature importance not available for {best_classification_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Regression Models\n",
    "\n",
    "Now, let's train regression models to predict trial duration for completed trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regression models\n",
    "regression_results = train_regression_models(X_regression, y_regression)\n",
    "\n",
    "# Display performance metrics\n",
    "regression_metrics = pd.DataFrame(regression_results['metrics'])\n",
    "regression_metrics = regression_metrics.sort_values('r2_score', ascending=False)\n",
    "\n",
    "print(\"\nRegression Model Performance:\")\n",
    "display(regression_metrics)\n",
    "\n",
    "# Identify best model\n",
    "best_regression_model_name = regression_metrics.index[0]\n",
    "best_regression_model = regression_results['models'][best_regression_model_name]\n",
    "\n",
    "print(f\"\nBest regression model: {best_regression_model_name}\")\n",
    "\n",
    "# Save the best model\n",
    "model_info = {\n",
    "    'model': best_regression_model,\n",
    "    'preprocessor': regression_results['preprocessor'],\n",
    "    'feature_names': feature_cols,\n",
    "    'metrics': regression_metrics.loc[best_regression_model_name].to_dict()\n",
    "}\n",
    "\n",
    "save_model(model_info, 'regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Regression Results\n",
    "\n",
    "Let's visualize the performance of our regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model performance comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot R² score\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=regression_metrics.index, y=regression_metrics['r2_score'])\n",
    "plt.title('R² Score by Model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('R² Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Plot RMSE\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x=regression_metrics.index, y=regression_metrics['rmse'])\n",
    "plt.title('RMSE by Model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('RMSE (days)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance for Regression\n",
    "\n",
    "Let's examine which features are most important for predicting trial duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from the best model (if available)\n",
    "if hasattr(best_regression_model, 'feature_importances_'):\n",
    "    # Get feature importances\n",
    "    importances = best_regression_model.feature_importances_\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    if hasattr(regression_results['preprocessor'], 'get_feature_names_out'):\n",
    "        feature_names = regression_results['preprocessor'].get_feature_names_out()\n",
    "    else:\n",
    "        feature_names = feature_cols\n",
    "    \n",
    "    # Create a DataFrame for visualization\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot top 20 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "    plt.title(f'Top 20 Important Features for {best_regression_model_name}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Feature importance not available for {best_regression_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've trained and evaluated models for predicting oncology clinical trial outcomes:\n",
    "\n",
    "1. **Classification Models for Trial Completion Status**\n",
    "   - Trained multiple models to predict whether a trial will complete successfully or terminate early\n",
    "   - Evaluated models using accuracy, precision, recall, and F1 score\n",
    "   - Identified the most important features for predicting completion status\n",
    "\n",
    "2. **Regression Models for Trial Duration**\n",
    "   - Trained multiple models to predict the duration of completed trials\n",
    "   - Evaluated models using R², MAE, and RMSE\n",
    "   - Identified the most important features for predicting trial duration\n",
    "\n",
    "The best models have been saved and can be used for further analysis and prediction in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}